{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SoQYfJ0Cm91R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#activation function (unti step function)\n",
        "def activation(x):\n",
        "  return 1 if x >= 0 else 0"
      ],
      "metadata": {
        "id": "lZ7JarB_Qffl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perceptron\n",
        "def perceptron(input , expected_output):\n",
        "  global w0 , w1 , b\n",
        "  w0 = random.uniform(-1,1)\n",
        "  w1 = random.uniform(-1,1)\n",
        "  b = random.uniform(-1,1)\n",
        "\n",
        "  print(\"Initilized weights and bias are : \",w0 , w1 , b)\n",
        "  print()\n",
        "\n",
        "  epochs = 100\n",
        "  lr = 0.1\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "    for i in range(len(input)):\n",
        "      X = input[i]\n",
        "      y = expected_output[i]\n",
        "\n",
        "      z = w0 * X[0] + w1 * X[1] + b\n",
        "      y_pred = activation(z)  #forward pass\n",
        "\n",
        "      error = y - y_pred # loss\n",
        "\n",
        "      w0 = w0 + lr * error * X[0]\n",
        "      w1 = w1 + lr * error * X[1]\n",
        "      b = b + lr * error\n",
        "\n",
        "      print(f'{i}/{epoch}: w0={w0}, w1={w1},  b={b}')\n",
        "      total_error += abs(error)\n",
        "\n",
        "    print()\n",
        "    if total_error == 0:\n",
        "      print(f\"Training completed in {epoch+1} epochs.\")\n",
        "      break\n",
        "\n",
        "  print(\"After trainig the weights and bias are : \",w0 , w1 , b)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NQd_oQHvQthJ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "def testing(input):\n",
        "  X = input\n",
        "  predicted = activation(w0 * X[0] + w1 * X[1] + b)\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "HN5TXqopSBiO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inputs and Expected_output\n",
        "AND_inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "AND_expected_outputs = [0, 0, 0, 1]\n",
        ""
      ],
      "metadata": {
        "id": "8tTeOhTTVCit"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the AND Gate\n",
        "perceptron(AND_inputs , AND_expected_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "H131hMd4VgXL",
        "outputId": "74603556-e6f6-40a2-b196-9eec3ef79e6a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initilized weights and bias are :  -0.4601791794312613 -0.47979655559903733 0.43037783061600465\n",
            "\n",
            "0/0: w0=-0.4601791794312613, w1=-0.47979655559903733,  b=0.3303778306160047\n",
            "1/0: w0=-0.4601791794312613, w1=-0.47979655559903733,  b=0.3303778306160047\n",
            "2/0: w0=-0.4601791794312613, w1=-0.47979655559903733,  b=0.3303778306160047\n",
            "3/0: w0=-0.3601791794312613, w1=-0.37979655559903736,  b=0.43037783061600465\n",
            "\n",
            "0/1: w0=-0.3601791794312613, w1=-0.37979655559903736,  b=0.3303778306160047\n",
            "1/1: w0=-0.3601791794312613, w1=-0.37979655559903736,  b=0.3303778306160047\n",
            "2/1: w0=-0.3601791794312613, w1=-0.37979655559903736,  b=0.3303778306160047\n",
            "3/1: w0=-0.26017917943126134, w1=-0.2797965555990374,  b=0.43037783061600465\n",
            "\n",
            "0/2: w0=-0.26017917943126134, w1=-0.2797965555990374,  b=0.3303778306160047\n",
            "1/2: w0=-0.26017917943126134, w1=-0.37979655559903736,  b=0.23037783061600467\n",
            "2/2: w0=-0.26017917943126134, w1=-0.37979655559903736,  b=0.23037783061600467\n",
            "3/2: w0=-0.16017917943126134, w1=-0.2797965555990374,  b=0.3303778306160047\n",
            "\n",
            "0/3: w0=-0.16017917943126134, w1=-0.2797965555990374,  b=0.23037783061600467\n",
            "1/3: w0=-0.16017917943126134, w1=-0.2797965555990374,  b=0.23037783061600467\n",
            "2/3: w0=-0.26017917943126134, w1=-0.2797965555990374,  b=0.13037783061600466\n",
            "3/3: w0=-0.16017917943126134, w1=-0.17979655559903737,  b=0.23037783061600467\n",
            "\n",
            "0/4: w0=-0.16017917943126134, w1=-0.17979655559903737,  b=0.13037783061600466\n",
            "1/4: w0=-0.16017917943126134, w1=-0.17979655559903737,  b=0.13037783061600466\n",
            "2/4: w0=-0.16017917943126134, w1=-0.17979655559903737,  b=0.13037783061600466\n",
            "3/4: w0=-0.06017917943126133, w1=-0.07979655559903737,  b=0.23037783061600467\n",
            "\n",
            "0/5: w0=-0.06017917943126133, w1=-0.07979655559903737,  b=0.13037783061600466\n",
            "1/5: w0=-0.06017917943126133, w1=-0.17979655559903737,  b=0.03037783061600466\n",
            "2/5: w0=-0.06017917943126133, w1=-0.17979655559903737,  b=0.03037783061600466\n",
            "3/5: w0=0.039820820568738674, w1=-0.07979655559903737,  b=0.13037783061600466\n",
            "\n",
            "0/6: w0=0.039820820568738674, w1=-0.07979655559903737,  b=0.03037783061600466\n",
            "1/6: w0=0.039820820568738674, w1=-0.07979655559903737,  b=0.03037783061600466\n",
            "2/6: w0=-0.06017917943126133, w1=-0.07979655559903737,  b=-0.06962216938399535\n",
            "3/6: w0=0.039820820568738674, w1=0.02020344440096264,  b=0.03037783061600466\n",
            "\n",
            "0/7: w0=0.039820820568738674, w1=0.02020344440096264,  b=-0.06962216938399535\n",
            "1/7: w0=0.039820820568738674, w1=0.02020344440096264,  b=-0.06962216938399535\n",
            "2/7: w0=0.039820820568738674, w1=0.02020344440096264,  b=-0.06962216938399535\n",
            "3/7: w0=0.13982082056873868, w1=0.12020344440096264,  b=0.03037783061600466\n",
            "\n",
            "0/8: w0=0.13982082056873868, w1=0.12020344440096264,  b=-0.06962216938399535\n",
            "1/8: w0=0.13982082056873868, w1=0.02020344440096264,  b=-0.16962216938399535\n",
            "2/8: w0=0.13982082056873868, w1=0.02020344440096264,  b=-0.16962216938399535\n",
            "3/8: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.06962216938399535\n",
            "\n",
            "0/9: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.06962216938399535\n",
            "1/9: w0=0.23982082056873869, w1=0.02020344440096264,  b=-0.16962216938399535\n",
            "2/9: w0=0.13982082056873868, w1=0.02020344440096264,  b=-0.26962216938399536\n",
            "3/9: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.16962216938399535\n",
            "\n",
            "0/10: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.16962216938399535\n",
            "1/10: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.16962216938399535\n",
            "2/10: w0=0.13982082056873868, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "3/10: w0=0.23982082056873869, w1=0.22020344440096265,  b=-0.16962216938399535\n",
            "\n",
            "0/11: w0=0.23982082056873869, w1=0.22020344440096265,  b=-0.16962216938399535\n",
            "1/11: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "2/11: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "3/11: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "\n",
            "0/12: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "1/12: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "2/12: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "3/12: w0=0.23982082056873869, w1=0.12020344440096264,  b=-0.26962216938399536\n",
            "\n",
            "Training completed in 13 epochs.\n",
            "After trainig the weights and bias are :  0.23982082056873869 0.12020344440096264 -0.26962216938399536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_data in AND_inputs:\n",
        "    print(f\"Input: {input_data}, Predicted Output: {testing(input_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QmuE5DOqVjOH",
        "outputId": "74bb3cf4-2770-4183-cf3b-d9a8f3673697"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: (0, 0), Predicted Output: 0\n",
            "Input: (0, 1), Predicted Output: 0\n",
            "Input: (1, 0), Predicted Output: 0\n",
            "Input: (1, 1), Predicted Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_i_7Ld8cdpWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}